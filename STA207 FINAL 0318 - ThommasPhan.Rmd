---
title: "Investigating the Impact of Class Size on Scaled Math Scores: An ANOVA Analysis of the STAR Dataset"
author: "Thommas Phan"
date: "March 18, 2024"
output:
  html_document:
    df_print: paged
    number_sections: yes
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

# Abstract

The Student/Teacher Achievement Ratio (STAR) study, conducted by the Tennessee State Department of Education in the 1980s, aimed to investigate the impact of class sizes on student achievement in American K-12 education. With over 7000 students across 79 schools, the study explored various class setups, including small classes, regular classes, and regular classes with a teacher's aide. This paper examines the randomized assignment process and key features of the experiment to unravel statistically significant differences in student scores across these class types, measured via Stanford Achievement Tests. The analysis focuses on first-grade math scores and employs exploratory data analysis, ANOVA modeling, and causal inference techniques to draw conclusions. Despite some violations of causal inference assumptions, particularly regarding randomization and treatment consistency, the study finds that small class sizes are associated with higher first-grade math scores, shedding light on the importance of class size in student outcomes.


# Introduction

In the landscape of American K-12 education, the size of classrooms holds considerable weight in shaping student outcomes. Acknowledging this, the State Department of Education in Tennessee undertook a pivotal initiative in the 1980sâ€”the Student/Teacher Achievement Ratio (STAR) study. With over 7000 students in 79 schools participating, this four-year longitudinal investigation set out to unravel the impact of class sizes on student achievement from kindergarten to third grade. The study encompassed various class setups, including small classes, regular classes, and regular classes with a teacher's aide. As we delve into the intricacies of the STAR project, examining the randomized assignment process and key features of the experiment, our primary goal is to unravel statistically significant differences in student scores across these distinct class types. By understanding these nuances, we aim to shed light on if class size actually makes a difference in scaled student outcomes, measured via Stanford Achievement Tests. 
 
# Background 

Understanding the impact of class sizes on student achievement is a pivotal concern for policymakers in the American K-12 education system. To research the effects of class size on student achievement in primary school, the State Department of Education in Tenessee launched a four-year longitudinal class-size randomized study in the 1980's called The Student/Teacher Achievement Ratio (STAR). Over 7000 students in 79 schools participated in this study. Some of the many features of the experiment process are listed below:

* All participating schools agreed to random assignment of teachers and students to different class conditions: small class (13-17 students per teacher), regular class (22-25 students per teacher), and regular-with-aide class (22-25 students per teacher with a full-time teacher's aide)

* Students were assigned to different class types from kindergarten until third grade.

* Each school must have enough kindergarten students to be assign all three class types in order to participate.

* Student achievement is measured annually with Stanford Achievement Tests (SATs) in Spring.

* If students switch from one STAR school to another, they will stay in the same type of class. Regular class sizes have the potential to become small as the small class type when students leave participating schools.

* Three schools withdrew from the project STAR at the end of kindergarten, leaving 76 schools in the 1st grade level. 

* This report will look into the following variables of interest at the first grade level: Class Type, Gender, Math Score, Race, School ID, Urbanicity, Teacher Career Ladder, Teacher Gender, Teacher Highest Degree, Teacher ID, Teacher Race, and Teaching Experience.

Considering the background information of project STAR, the natural question of interest to ask is whether there are any statistically significant differences in scores across class types, and if so, which class type is associated with the highest scores? 

# Descriptive analysis 

**Question of Interest:**

Are there differences in first grade scaled math scores across class types? If so, which class type is associated with the highest scaled first grade math scores?

## Exploratory Data Analysis

The raw STAR dataset has 11601 observations with 379 attributes. It describes the demographics of the students and teachers, class type assignment, the school's and classroom's identification number, test score values per student, as well as many other variables, listed in the background. 

We will begin our analysis with a few frequency plots of variables of interest, including a first grade scaled math score frequency histogram to determine what our summary measures should be. Before we evaluate the data, we will clean the dataset such that any rows of data that have missing NA values for our variables of interest will be removed, as they are incomplete. There are also 4 schools that do not have all 3 class types, which will be removed. 

```{r load all libraries, include=FALSE}
# Load packages
library(dplyr)
library(ggplot2)
library(gplots)
library(patchwork)
library(AER)
library(haven)
library(tidyverse)
library(stats)
library(car)
```



```{r, results='hide',echo=FALSE, message=FALSE}
star <- suppressMessages(read_sav("STAR_Students.sav"))

star$gender <- factor(star$gender, labels = c("male", "female"))
star$race <- factor(star$race, labels = c("caucasian", "black", "asian", "hispanic", "native", "other"))
star$g1surban <- factor(star$g1surban, labels = c("inner city", "suburban", "rural", "urban"))
star$gkclasstype <- factor(star$gkclasstype, labels = c("small", "regular", "regular + aide"))
star$g1classtype <- factor(star$g1classtype, labels = c("small", "regular", "regular + aide"))
star$g1tgen <- factor(star$g1tgen, labels = c("male", "female"))
star$g1trace <- factor(star$g1trace, labels = c("caucasion", "black"))
star$g1thighdegree <- factor(star$g1thighdegree, labels = c("bach", "masters","special", "dr"))

```


<center>
<strong><h4>**Figure 4.1: Distribution of Valid and NA Values in STAR Dataset**</strong></h4>
</center>


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=8, fig.height=3}

# Keep the columns only relevant to the first grade students
columns <- c("gender","race","g1classtype","g1schid","g1surban","g1tchid","g1tgen","g1trace","g1thighdegree","g1tcareer","g1tyears","g1tmathss")
STAR <- star[,columns]

# Change the colnames
colnames(STAR) <- c("Gender","Race", "Class Type in Grade 1", "School ID","School Urbanicity","Teacher ID", "Teacher Gender", "Teacher Race", "Teacher Highest Degree", "Teacher Career Ladder","Teaching Experience", "Math Scale Score in 1st Grade")

STAR %>%
  gather(key = "Variable", value = "Value") %>%
  group_by(Variable) %>%
  summarise(Valid = sum(!is.na(Value)), NA_Values = sum(is.na(Value))) %>%
  gather(key = "Status", value = "Count", Valid, NA_Values) %>%
  
  ggplot(aes(x = Variable, y = Count, fill = Status)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(#title = "Distribution of Valid and NA Values in STAR Dataset",
       x = "Variable",
       y = "Proportion",
       fill = "Status") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

As demonstrated in Figure 4.1, there is a substantial amount of data that is incomplete that must be removed. Figure 4.2 show the distributions of students in the cleaned data of variables of interest, which will then be grouped by teachers. The decision to group by teachers was discussed in the Initial Analysis report; In brief, using student scores violates the Stable Unit Treatment Value Assumption (SUTVA) when trying to make causal inference, since students are prone to peer influences in their own classroom. We expect the average of test scores by teacher to control for the randomization of teacher assignment not affecting potential outcomes of others in the same group. This is covered further in section 7.1.

<center>
<strong><h4>**Figure 4.2: Student Distributions**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=3}
# Filter out missing values
columns_to_filter <- c("Gender", "Race", "Class Type in Grade 1", "School ID", 
                       "School Urbanicity", "Teacher ID", "Teacher Gender", 
                       "Teacher Race", "Teacher Highest Degree", 
                       "Teacher Career Ladder", "Teaching Experience", 
                       "Math Scale Score in 1st Grade")

STAR_filtered <- na.omit(STAR[columns_to_filter])

# Treat categorical variables as factors
columns_to_factorize <- c("Gender","Race", "Class Type in Grade 1", "School ID", 
                           "School Urbanicity", "Teacher ID", "Teacher Gender", 
                           "Teacher Race", "Teacher Highest Degree", 
                           "Teacher Career Ladder")

# Creating STAR_filtered with NAs removed and specified columns treated as factors
STAR_filtered <- STAR %>%
  na.omit() %>%
  mutate(across(all_of(columns_to_factorize), as.factor))

STAR_filtered <- STAR_filtered %>%
  rename(
    Gender = `Gender`,
    Race = `Race`,
    Class_Type_Grade_1 = `Class Type in Grade 1`,
    School_ID = `School ID`,
    School_Urbanicity = `School Urbanicity`,
    Teacher_ID = `Teacher ID`,
    Teacher_Gender = `Teacher Gender`,
    Teacher_Race = `Teacher Race`,
    Teacher_Highest_Degree = `Teacher Highest Degree`,
    Teacher_Career_Ladder = `Teacher Career Ladder`,
    Teaching_Experience = `Teaching Experience`,
    Math_Scale_Score_1st_Grade = `Math Scale Score in 1st Grade`
  )

# Remove School IDs that are missing all 3 class types
school_ids_to_remove <- c(244728, 244796, 244736, 244839)

# Remove rows with specified School_ID values
STAR_filtered <- STAR_filtered[!STAR_filtered$School_ID %in% school_ids_to_remove, ]


# Create individual plots for each variable
plot_v1 <- ggplot(STAR_filtered, aes(x = Gender)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Gender Distribution", x = "Gender", y = "Count")+
  theme_minimal()

plot_v2 <- ggplot(STAR_filtered, aes(x = Race)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Ethnicity Distribution", x = "Ethnicity", y = "Count")+
  theme_minimal()

plot_v3 <- ggplot(STAR_filtered, aes(x = School_Urbanicity)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Urbanicity Distribution", x = "Urbanicity", y = "Count")+
  theme_minimal()

plot_v4 <- ggplot(STAR_filtered, aes(x = Class_Type_Grade_1)) +
  geom_bar(fill = "lightblue") +
  labs(title = "Class Type Distribution", x = "Class Type", y = "Count")+
  theme_minimal()

plot_v5 <- ggplot(STAR_filtered, aes(x = Math_Scale_Score_1st_Grade)) +
  geom_histogram(fill = "lightblue",color="black", bins = 30) +
  geom_vline(aes(xintercept = mean(Math_Scale_Score_1st_Grade, na.rm = TRUE)),
             color = "red", linetype = "dashed", size = 1) +
  labs(title = "First Grade Math Scores",
       x = "Math1", y = "Count")+
  theme_minimal()

# Arrange plots using patchwork
plots_combined <- (plot_v1 / plot_v3) | (plot_v2 / plot_v4) | plot_v5

plots_combined
```


After cleaning, there are 323 observations (337 before cleaning) of teachers, with the following distribution by classroom size. Regular: 106, Small: 117, Regular+Aide: 100. The frequency histogram shows that math1 is slightly right skewed, but close enough to normal that either mean or median would be an appropriate summary measure. For this analysis report, we will use mean as the summary statistic, which will capture more information of the distribution. A deeper look into the decision between mean and median is explored in Section 6.1. We will begin EDA of the data at a teacher level.

<center>
<h4><strong>**Figure 4.3: Teacher Distributions**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=11, fig.height=4}
STAR_by_teacher <- STAR_filtered %>%
  group_by(Teacher_ID) %>%
  summarise(Teacher_Math_Score = mean(Math_Scale_Score_1st_Grade, na.rm = TRUE)) %>%
  filter(!is.na(Teacher_ID))

STAR_by_teacher <- STAR_by_teacher %>%
  left_join(STAR_filtered %>%
              select(Teacher_ID,
                     Class_Type_Grade_1,
                     School_ID,
                     School_Urbanicity,
                     Teacher_Gender,
                     Teacher_Race,
                     Teacher_Highest_Degree,
                     Teacher_Career_Ladder,
                     Teaching_Experience) %>%
              distinct(),  # Keep only unique combinations of Teacher_ID
            by = "Teacher_ID")


teacher_counts <- STAR_by_teacher %>%
  group_by(School_ID) %>%
  summarise(number_of_teachers = n_distinct(Teacher_ID))

# Plotting with ggplot2
plot_t4.5 <-ggplot(teacher_counts, aes(x = number_of_teachers)) +
  geom_histogram(binwidth = 1, fill = "coral", color = "black", alpha = 1) +
  labs(title = "Distribution of Number of Teachers per School",
       x = "Number of Teachers",
       y = "Frequency") +
  theme_minimal()



# Create individual plots for each Teacher variable
plot_t1 <- ggplot(STAR_by_teacher, aes(x = Teacher_Gender)) +
  geom_bar(fill = "coral") +
  labs(title = "Teacher Gender Distribution", x = "Teacher_Gender", y = "Count")+
  theme_minimal()
  

plot_t2 <- ggplot(STAR_by_teacher, aes(x = Teacher_Race)) +
  geom_bar(fill = "coral") +
  labs(title = "Teacher Ethnicity Distribution", x = "Teacher Ethnicity", y = "Count")+
  theme_minimal()

plot_t3 <- ggplot(STAR_by_teacher, aes(x = Teacher_Highest_Degree)) +
  geom_bar(fill = "coral") +
  labs(title = "Teacher Degree Distribution", x = "Degree Type", y = "Count")+
  theme_minimal()

plot_t4 <- ggplot(STAR_by_teacher, aes(x = Teacher_Career_Ladder)) +
  geom_bar(fill = "coral") +
  labs(title = "Teacher Career Distribution", x = "Career Ladder", y = "Count")+
  theme_minimal()

plot_t5 <- ggplot(STAR_by_teacher, aes(x = Teaching_Experience)) +
  geom_histogram(fill = "coral",color="black", bins = 30) +
  geom_vline(aes(xintercept = median(Teaching_Experience, na.rm = TRUE)),
             color = "blue", linetype = "dashed", size = 1) +
  labs(title = "Teacher Experience Distribution",
       x = "Math1", y = "Count")+
  theme_minimal()

plot_t6 <- ggplot(STAR_by_teacher, aes(x = Teacher_Math_Score)) +
  geom_histogram(fill = "coral", color="black", bins = 30) +
  geom_vline(aes(xintercept = mean(Teacher_Math_Score, na.rm = TRUE)),
             color = "blue", linetype = "dashed", size = 1) +
  labs(title = "Teacher Mean Math Score Distribution",
       x = "Math1", y = "Count")+
  theme_minimal()

teacher_plots_combined1 <- (plot_t1 | plot_t2) / (plot_t3 |plot_t4) / plot_t4.5
teacher_plots_combined2 <- plot_t5 / plot_t6

teacher_plots_combined1 | teacher_plots_combined2
```

Figure 4.3 demonstrates that the teacher gender, ethnicity, highest degree, and career ladder distribution are not random. This is due to the experiment design not allowing to have perfect samples of teachers since most teachers in first grade are female and caucasion in Tenessee. Teacher mean math scores are approximately normal, with a slight right skew. Some variables to note is the inconsistent distribution of number of teachers per school, and teacher experience being right skewed.


<center>
<h4><strong>**Figure 4.4: Violin and Boxplots of Math Scores by Class Type and School**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=4}
medians_by_schoolid <- STAR_filtered %>%
  group_by(School_ID) %>%
  summarise(median_math1 = median(Math_Scale_Score_1st_Grade, na.rm = TRUE)) %>%
  arrange(median_math1)

boxplot_plot <- ggplot(STAR_filtered, aes(x = factor(School_ID), y = Math_Scale_Score_1st_Grade)) +
  geom_boxplot(fill = "lightblue", color = "black", alpha = 0.7) +
  labs(title = "First Grade Math Scores by School",
       x = "School (Ranked by Median Math Score)",
       y = "Math Score") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

boxplot_plot <- boxplot_plot + scale_x_discrete(limits = medians_by_schoolid$School_ID)

# Calculate means by Class_Type_Grade_1
means_by_class_type <- STAR_filtered %>%
  group_by(Class_Type_Grade_1) %>%
  summarise(mean_math_score = mean(Math_Scale_Score_1st_Grade, na.rm = TRUE))

# Create a violin plot with means
plot_violin <- ggplot(STAR_filtered, aes(x = Class_Type_Grade_1, y = Math_Scale_Score_1st_Grade, fill = Class_Type_Grade_1)) +
  geom_violin(trim = TRUE) +
  geom_boxplot(width = 0.2, fill = "white", color = "black") +
  geom_point(data = means_by_class_type, aes(x = Class_Type_Grade_1, y = mean_math_score),
             color = "red", size = 3) +
  labs(title = "First Grade Math Scores by Class Size",
       x = "Class Type in Grade 1",
       y = "Math Scale Score in 1st Grade") +
  theme_minimal()

# Display the plot
 plot_violin | boxplot_plot
```

The Violin Plot above demonstrates that each of the class size distributions appear approximately normal, but have differing means/medians. The boxplots of First Grade Math Scores by School (ranked by median, lowest to highest) demonstrate that there is a large variation between student performance and the school itself.  These two variables will looked further into in our Inferential Analysis, section 5.


# Inferential analysis 

## Analysis Plan: The Two-Way ANOVA Model Selection
Based on the background research of the STAR project, we will define a tentative, "full" initial Two-Way ANOVA model to be 

$$Y_{ijk} = \mu_{..} + \alpha_{i} + \beta_{j} + (\alpha\beta)_{ij} + \epsilon_{ijk}$$

where the index $i$ represents the class type: small ($i=1$), regular ($i=2$), regular with aide ($i=3$), and the index $j$ represents the school indicator. 

### Model Explanation of Notation and Assumptions

$Y_{ijk}$ is the outcome, in this case the individual teacher's mean of student's first grade math score.

$\alpha_{i}$ is the effect of class type (i) on the outcome.

$\beta_{j}$ is the effect of the school (j) on the outcome. 

$(\alpha\beta)_{ij}$ is the interaction effect of both the class type and the school. 

$\epsilon_{ijk}$ is the noise of the model, with assumptions that $\epsilon_{ijk}\stackrel{i.i.d.}{\sim}{N(0,\sigma^2)}$. We will evaluate if this is a fair assumption later in this report's sensitivity analysis.

We've tentatively decided to choose this specific model because background research indicates that the variable of interest is class size. Since there is evidence that School identification is a good predictor of math performance, we've decided to also include this as $\beta_j$. In general, it is considered good practice to include both the school id and the interaction effect of both school id and the class size; while the true variable of interest is the class size, we want to remove as much variability as possible that could be attributed to both the school as well as the school's ability to implement class size. However, since we are grouping by the mean math scores of each teacher, we can expect the interactive effects between school and class size to be mitigated. We will run initial tests to see if this is true. 

### Model Reduction

We are interested in testing the presence of interactions, to see if it is justified to include the interaction terms in our ANOVA model. Below will be the null and alternative hypothesis.

$$
H_0: (\alpha\beta)_{ij}=0 \ {\rm v.s.} \ H_1: {\rm not \ all \ } (\alpha\beta)_{ij} \ {\rm are \ zero}.
$$

We will test with the following framework:

* Full model: $Y_{ijk} =\mu_{\cdot\cdot} + \alpha_i+\beta_j + (\alpha\beta)_{ij}+\epsilon_{ijk}$

* Reduced model: $Y_{ijk} =\mu_{\cdot\cdot} + \alpha_i+\beta_j +\epsilon_{ijk}$

The F-statistic is then 
$$
F^*=\frac{ [{\rm SSE}_{\rm red}-{\rm SSE}_{\rm full}  ] / [ df_{\rm red}-df_{\rm full}   ]  }{ {\rm SSE}_{\rm full}/df_{\rm full}},
$$
where $F^* \sim F( (a-1)(b-1), n_T-ab)$ under the null hypothesis. 

<center>
<h4><strong>**Table 5.1: Full vs Reduced Model**</strong></h4>
</center>

```{r, echo=FALSE}
# Fit the full and reduced models
full_model <- lm(Teacher_Math_Score ~ as.factor(Class_Type_Grade_1) + 
                    as.factor(School_ID) + 
                    as.factor(Class_Type_Grade_1) * as.factor(School_ID), 
                  data = STAR_by_teacher)
reduced_model <- lm(Teacher_Math_Score ~ as.factor(Class_Type_Grade_1) + 
                       as.factor(School_ID), 
                    data = STAR_by_teacher)

# Perform ANOVA and extract the results
anova_results <- anova(reduced_model, full_model)

# Convert ANOVA results to dataframe
anova_df <- as.data.frame(anova_results)

# Rename columns for better readability
names(anova_df) <- c("Source", "Sum of Squares", "Degrees of Freedom", 
                     "Mean Squares", "F value", "Pr(>F)")

# Print the dataframe
anova_df
```


## Final Model Justification

From Table 5.1, we find that the interaction effect when using teacher's outcome of mean first grade math scores is insignificant, and we can sufficiently assume that a reduced model would serve our purposes better, since the effects would be marginal from the interaction effects. For ease of interpretation, we will continue our analysis with the reduced model. We know school ID to be a valid addition to the model from the Initial Analysis report, where we showed through a one-way ANOVA that the means are statistically different, suggesting that there is a large source of variability here. 

We will define the final, reduced Two-Way ANOVA model to be 

$$Y_{ijk} = \mu_{..} + \alpha_{i} + \beta_{j} +  \epsilon_{ijk}$$ 

where the index $i$ represents the class type: small ($i=1$), regular ($i=2$), regular with aide ($i=3$), and the index $j$ represents the school indicator. 

$Y_{ijk}$ is the outcome, in this case the individual teacher's mean of student's math1 score.

$\alpha_{i}$ is the effect of class type (i) on the outcome.

$\beta_{j}$ is the effect of the school (j) on the outcome. 

$\epsilon_{ijk}$ is the noise of the model, with assumptions that $\epsilon_{ijk}\stackrel{i.i.d.}{\sim}{N(0,\sigma^2)}$. 

Since the variable of interest is the class size's effect on students, the main report will not include the estimated coefficients for School_ID. All coefficients are reported in the Appendix, Section 9. School_ID was investigated through a one-way ANOVA in the initial analysis and confirmed a statistically significant variable when measuring student outcome in math scores. A brief analysis of the $\beta_j$s are included in the appendix, Section 9.

### Model Diagnostics

The final model makes the assumption that the error terms both have equal variance and are normally distributed. Figure 5.1 shows the diagnostic plots of the residuals vs fitted values and the QQ plot of the residuals, which shows that the residuals appear uncorrelated and independent, while the normality assumption is violated because there is a heavy tail. This non-normality is proven in section 5.3 with a Shapiro Wilk test for normality, with a significant p-value. 

A box-cox transformation was considered and tested on the response variable in section 5.3, but a transformed response variable still is significant. Since a log transformation undermines the interpretability of results, we will continue on with the original data, and assume normality.

<center>
<h4><strong>**Figure 5.1: Diagnostic Plots of Final Model**</strong></h4>
</center>
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=7, fig.height=4}
# Convert Class_Type_Grade_1 and School_ID to factors if they are not already
STAR_by_teacher$Class_Type_Grade_1 <- as.factor(STAR_by_teacher$Class_Type_Grade_1)
STAR_by_teacher$School_ID <- as.factor(STAR_by_teacher$School_ID)

# Perform ANOVA
anova.fit <- aov(Teacher_Math_Score ~ Class_Type_Grade_1 + School_ID, data = STAR_by_teacher)

# Set up the layout
par(mfrow = c(1, 2))

# Plot the ANOVA results
plot(anova.fit, which = 1) 
plot(anova.fit, which = 2)
```

### Model Hypothesis Testing

We will fit the new model to test the following hypothesis, with the conservative significance level $\alpha=0.01$, to reduce possibility of type I errors. 
$$
H_0: \alpha_{i}=0 \ \ \forall i \ \ \  {\rm v.s.}\ \  \ H_1:\  {\rm not \ all \ } \alpha_i \ {\rm are \ zero}.
$$


We are only testing for $\alpha=0$ because the question of interest is what the effect of class size has on a teacher's classroom's math1 scores. Since the number of teachers by School_ID are imbalanced, there is reason to use a Type II ANOVA since there are imbalanced group sizes and there are no interaction terms. 

However, since we are interested in the effect in solely the class size type, we will proceed with the assumption that the class sizes are balanced and interpret the results through a Type I ANOVA for the following reasons: Type I ANOVA sequenced with categorical variable class size first will give more insight of the individual contribution to math1 scores, the experimental design documented that each teacher/student was randomly assigned to the class size, and the coefficients of interest will only be from the categorical variable of class-type. As a result, from initial analysis, it is reasonable to believe that a Type I ANOVA will fit our purposes best. Consideration for a Type II and III ANOVA were tested in the initial analysis report and came to similar conclusions.

## Results

<center>
<h4><strong>**Table 5.2: ANOVA Table from Final Model**</strong></h4>
</center>

```{r, echo=FALSE}
# Ensure Class_Type_Grade_1 is a factor
STAR_by_teacher$Class_Type_Grade_1 <- as.factor(STAR_by_teacher$Class_Type_Grade_1)

# Recode the factor levels
STAR_by_teacher$Class_Type_Grade_1 <- recode_factor(STAR_by_teacher$Class_Type_Grade_1,
                                                     "1" = "small",
                                                     "2" = "regular",
                                                     "3" = "regular + aide")

# Perform ANOVA
anova_fit <- aov(Teacher_Math_Score ~ Class_Type_Grade_1 + School_ID, data = STAR_by_teacher)

# Get ANOVA summary
anova_summary <- summary(anova_fit)

# Specify significance level
sig.level <- 0.01


# Extract ANOVA table
anova_table <- as.data.frame(anova_summary[[1]])

# Print the dataframe
anova_table
```

We see from the summary Table 5.2 of the Two-Way ANOVA model is very significant for each variable included, suggesting there is reason to believe each of the $\alpha_{i}, \beta_{j}$ are not all zero. Since we are only interested in the effect of class size type, only the coefficients of $\alpha_i$ are reported.

<center>
<h4><strong>**Table 5.3: Coefficients of Final Model**</strong></h4>
</center>

```{r, echo=FALSE}
# Convert ANOVA coefficients to a dataframe
coefficients_df <- as.data.frame(anova.fit$coefficients[1:3])

# Rename the second column
names(coefficients_df)[1] <- "Coefficients"

# Print the dataframe
coefficients_df
```

The coefficients reported in Table 5.3 can be interpreted as follows: Starting from the small class size, the treatment of "regular" class size is associated with the change of first grade math scores by $\alpha_2=-13.53$, while the treatment of regular+aide class type is associated with the change of first grade math scores of $\alpha_3 = -11.52$. The small class size was chosen as the "default" class size because, shown in section 7.1, the small class size was least affected by "unrandomization" of students moving within other class types after initial randomization. All these associations are holding the school identification ($\beta_j$) constant for each factor effect. Figure 5.2 demonstrates the mean factor effect of each treatment. 

<center>
<h4><strong>**Figure 5.2: Main Effects Plot on Mean Math Scores by Teacher and Class Size**</strong></h4>
</center>


```{r, echo=FALSE,warning=FALSE, message=FALSE, fig.align="center", fig.width=7, fig.height=4}
STAR_by_teacher$Class_Type_Grade_1=as.factor(STAR_by_teacher$Class_Type_Grade_1)

options(repr.plot.width=12, repr.plot.height=12)
  
plotmeans(Teacher_Math_Score ~ Class_Type_Grade_1, data = STAR_by_teacher, xlab = "Class_Type_Grade_1", ylab = "Teacher_Math_Score",
          #main = "Main Effects Plot on Mean Math1 by Teacher and Class Size", 
          cex.lab = 1.5)
```


Recall the violin plot from Figure 4.4, which is closely related to the Main Effects Plot, Figure 5.1. Both Figures shows us that 1) the number of teachers of each class size is approximately the same, 2) the variances seem equal, and 3) the means appear different. We will test if there is a statistically significant different using Tukey's range test, to determine if each pair-wise comparison is significantly different or not, at a significance level of 0.01 to avoid Type 1 error, since we are testing across three different combinations.

<center>
<h4><strong>**Figure 5.3: Tukey Confidence Intervals Across Pairwise Combinations of classes**</strong></h4>
</center>


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=8, fig.height=5}
T.ci=TukeyHSD(anova.fit, "Class_Type_Grade_1",conf.level = 1-sig.level)

# Adjust the size of y-axis text
par(cex.axis = 0.66)

# Plot the Tukey HSD results
plot(T.ci)
```

At a significance level $\alpha = 0.01$, Tukey's test gives us evidence that the small classroom size is significantly smaller than the other two class types. We can confirm this with a one-sided t-test, but first we must carry out a Levene test to test the variances' equality between levels: (This is explored further in the sensitivity analysis)

```{r, echo=FALSE}
# Calculate the variances for each group:
vars = tapply(STAR_by_teacher$Teacher_Math_Score,STAR_by_teacher$Class_Type_Grade_1,var)

alpha=0.05;

STAR_by_teacher$res.abs=abs(anova.fit$residuals);
summary(aov(res.abs~Class_Type_Grade_1,data=STAR_by_teacher))
```
The Levene test suggests there is not enough evidence to conclude the variances are unequal between groups, with a p-value of 0.0636. Since this is close to a typical level of significance of 0.05, it is reasonable to question either conclusion from this test. To be safe, we will carry out a t-test for both equal and unequal variances.

<center>
<h4><strong>**Table 5.4: Welch's T Test for Small vs Regular + Aide Class Type**</strong></h4>
</center>

```{r, echo=FALSE}
# Subset the data for each level
regular_aide <- STAR_by_teacher$Teacher_Math_Score[STAR_by_teacher$Class_Type_Grade_1 == "regular + aide"]
small <- STAR_by_teacher$Teacher_Math_Score[STAR_by_teacher$Class_Type_Grade_1 == "small"]

# Perform a t-test
t_test_result <- t.test(regular_aide, small, alternative = "less", var.equal=TRUE)
#print(t_test_result)

t_test_result2 <- t.test(regular_aide, small, alternative = "less", var.equal=FALSE)
#print(t_test_result2)
# Extract t, df, and p-value from the t-test results
result_data <- data.frame(
  Test = c("Equal Variance Assumed", "Equal Variance Not Assumed"),
  t_value = c(t_test_result$statistic, t_test_result2$statistic),
  df = c(t_test_result$parameter, t_test_result2$parameter),
  p_value = c(t_test_result$p.value, t_test_result2$p.value)
)

# Print the dataframe
result_data

```

We end up finding both the unequal and equal variance types to be significant, with p values of 0.001187 and 0.001259, respectively.  We can conclude that the small class size has a statistically significant effect on mean scaled first grade math scores when grouped by teachers.

## Sensitivity analysis 

### Diagnostic Plots

We will begin our sensitivity analysis by looking into the Residual vs. Fitted, QQ Plot, and Cook's Distance plots first, demonstrated in Figure 5.4.

<center>
<h4><strong>**Figure 5.4: Diagnostic Plots**</strong></h4>
</center>


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=3}
# Set up the layout
par(mfrow = c(1, 3))

# Plot ANOVA results
plot(anova.fit, which = 1)
plot(anova.fit, which = 2)
plot(anova.fit, which = 4)
```

**Residuals vs. Fitted Plot:**

The residuals vs. fitted plot reveals an apparent lack of systematic patterns or trends, suggesting that the relationship between the predictors and the response variable is adequately captured by the chosen model. The scattered distribution of residuals around zero across the range of fitted values indicates that there is no substantial evidence of heteroscedasticity or nonlinearity. The absence of discernible patterns in the residuals suggests that the assumption of independence is reasonable, supporting the validity of the model assumptions.

**QQ Plot:**

The quantile-quantile (QQ) plot shows a symmetric, but heavy tailed distribution. A Shapiro Wilk Test was used to show a significant p-value suggesting this data is not normally distributed. A box-cox transformation was considered, but a transformation of the response, or any variable, will undermine the interpretability of the model. For this model's purposes, we will continue on that the normality assumption is fair, since it is still symmetric. 

```{r, echo=FALSE}
# Extract residuals from the ANOVA model
residuals <- residuals(anova.fit)

# Perform Shapiro-Wilk test for normality
shapiro_test <- shapiro.test(residuals)

# Print the results
print(shapiro_test)
```

**Cook's Distance Plot:**

The Cook's Distance plot suggests that there are 3 outliers that may effect our model. The model was recreated excluding these samples and was found to have similar results. Considering a sample of >300, it is reasonable to justify to keep these furthest outliers since they do not make a substantial difference. 

**Overall Assessment:**

These diagnostic plots collectively provide evidence in favor of the appropriateness of the model assumptions. The residuals vs. fitted plot indicates independence, and the normal QQ plot suggests that the residuals follow an approximately normal distribution. While these findings contribute to the robustness of our analysis, it is essential to interpret these results in the context of the specific model and dataset. The observed patterns in the diagnostic plots support the reliability of our model for making inferences and predictions.

### Heterogeneity of the Population

We know from the Initial Analysis from One-Way and Two-Way ANOVA tests that the mean math scores by both class size and school ID are different (Class size was moved to inferential analysis for convenience to the reader). In this section, we will test if the variances are different via Levene Test.

**Levene Test for Unequal Variances between School ID and Math Scores**

```{r, echo=FALSE}
# Calculate the variances for each group:
vars = tapply(STAR_by_teacher$Teacher_Math_Score,STAR_by_teacher$School_ID,var)

alpha=0.05;

STAR_by_teacher$res.abs=abs(anova.fit$residuals);
summary(aov(res.abs~School_ID,data=STAR_by_teacher))
```
The Levene Test for Unequal Variances between School ID and Math scores suggests there is enough evidence to conclude there are unequal variances between the math scores and School ID. This is expected as there are 70 schools all of which appear to come from slightly different population spreads (both mean and variance) as demonstrated in Figure 4.4.

# Additional Exploration and Diagnostics

## Mean vs. Median Selection for Teacher's Math Scores

This section of the sensitivity analysis will evaluate if mean is the correct choice over median. Based on the descriptive plots and diagnostics, we expect median and mean to output similar results, and the initial model chooses mean over median to capture more information from the data.

<center>
<h4><strong>**Figure 6.1: Mean vs Median Distribution by Teacher**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=3}
# Make a new df with median instead of mean
STAR_by_teacher_median <- STAR_filtered %>%
  group_by(Teacher_ID, School_ID, Class_Type_Grade_1) %>%
  summarise(median_math1 = median(Math_Scale_Score_1st_Grade, na.rm = TRUE)) %>%
  filter(!is.na(Teacher_ID))

hist_plot <- ggplot(STAR_by_teacher, aes(x = Teacher_Math_Score)) +
  geom_histogram(fill = "lightblue", bins = 20, color = "black", alpha = 0.7) +
  labs(title = "Histogram of Mean Math Scores by Teacher",
       x = "Mean Math Score",
       y = "Frequency") +
  theme_minimal()

hist_plot2 <- ggplot(STAR_by_teacher_median, aes(x = median_math1)) +
  geom_histogram(fill = "orange", bins = 20, color = "black", alpha = 0.7) +
  labs(title = "Histogram of Median Math Scores by Teacher",
       x = "Median Math Score",
       y = "Frequency") +
  theme_minimal()

mean_vs_median <- (hist_plot | hist_plot2)

mean_vs_median
```

The distribution of mean vs median first grade math scores by teacher looks nearly the same, indicating there would be very little difference between the choice of the two. We will now check to see if our ANOVA results would have been different, first checking if interaction effects should have been included with median being used as a summary measure. 

<center>
<h4><strong>**Table 6.1: Full vs Reduced Model**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=5}
# Fit the full and reduced models
full_model2 <- lm(median_math1 ~ as.factor(Class_Type_Grade_1) + 
                     as.factor(School_ID) + as.factor(Class_Type_Grade_1) * as.factor(School_ID), 
                   data = STAR_by_teacher_median)
reduced_model2 <- lm(median_math1 ~ as.factor(Class_Type_Grade_1) + as.factor(School_ID),
                     data = STAR_by_teacher_median)

# Perform ANOVA and extract the results
anova_results2 <- anova(reduced_model2, full_model2)

# Convert ANOVA results to dataframe
anova_df2 <- as.data.frame(anova_results2)

# Print the dataframe
anova_df2
```
There is still evidence to remove the median's interaction effects, albeit less significant than when using the mean.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=5}
# Convert Class_Type_Grade_1 and School_ID to factors if they are not already
STAR_by_teacher_median$Class_Type_Grade_1 <- as.factor(STAR_by_teacher_median$Class_Type_Grade_1)
STAR_by_teacher_median$School_ID <- as.factor(STAR_by_teacher_median$School_ID)


sig.level=0.01;
anova.fit_median<-aov(median_math1~Class_Type_Grade_1+School_ID,data=STAR_by_teacher_median)

print(summary(anova.fit_median))

```

<center>
<h4><strong>**Table 6.2: Coefficients of Median Model**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=5}

coefficients_df_median <- as.data.frame(anova.fit_median$coefficients[1:3])

names(coefficients_df_median)[1] <- "Coefficients"

# Print the dataframe
coefficients_df_median
```

The results of a Type I ANOVA on both the mean and median are similar. However, the effect of regular+aide is associated with a larger negative affect than the regular class type. Lastly, we will look at the Tukey CI and sensitivity plots. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=5}
T.ci_median=TukeyHSD(anova.fit_median, "Class_Type_Grade_1",conf.level = 1-sig.level)
# Adjust the size of y-axis text
par(cex.axis = 0.66)
```

As demonstrated in Figure 6.2, The difference between regular vs. regular+aide is slightly less, but the difference between small classrooms vs others is still significant. Lastly, we will conduct sensitivity analysis to see if the assumptions that $\epsilon_{ijk}\stackrel{i.i.d.}{\sim}{N(0,\sigma^2)}$ is a better assumption for the median scores.

<center>
<h4><strong>**Figure 6.2: Tukey CI and Sensitivity Plots for Median**</strong></h4>
</center>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=3}
# Set up the layout
par(mfrow = c(1, 4))

# Plot ANOVA results
plot(T.ci_median)
plot(anova.fit_median, which = 1)
plot(anova.fit_median, which = 2)
plot(anova.fit_median, which = 4)
```

The residuals vs. fitted values plot looks very similar to the mean ANOVA fit. However, the median's QQ plot suggests closer to normal data, with lighter tails than the mean, albeit marginally more normal. The Cook's distance plot suggests the same thing as the median counterpart.

**Conclusion from Median Analysis**

Findings from a brief analysis of the median teacher score vs. mean teacher score showed the results to be very similar, with minor differences, such as the QQ plots of the two, with the cost of finding less variability caused by adding an aide to the classroom.

## Potential Alternative Variable Selection
A natural question to ask from a dataset this expansive is if there is a possible model selection that is "better" than the model chosen. We will implement Akaike's Information Criterion (AIC) to determine with the teacher dataframe to see if there are possible better selections than the one chosen.

\section{Akaike Information Criterion (AIC) Framework}

The Akaike Information Criterion (AIC) is a statistical measure used for model selection. It provides a balance between the goodness of fit of a model and its complexity, thereby assisting in the selection of the most appropriate model from a set of candidate models.

AIC is calculated using the formula:

\[
\text{AIC} = -2 \times log(\frac{SSE_p}{n}) + 2p
\]

where:
\begin{itemize}
\item \textbf{log(\frac{SSE_p}{n})} is the maximized value of the likelihood function of the model.
\item \textbf{p} is the number of parameters in the model.
\end{itemize}

The AIC score penalizes models with a larger number of parameters, encouraging the selection of simpler models that still adequately explain the data. Lower AIC values indicate better fitting models.

In this analysis, we will employ the AIC framework to select the most suitable model for predicting the response variable (Teacher\_Math\_Score) based on all predictor variables. The stepwise selection process helps us systematically add or remove predictor variables from the model while optimizing the AIC score, ensuring that the selected model strikes an appropriate balance between goodness of fit and model complexity. Below is the result from the AIC selection process. 


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=10, fig.height=5}
# Assuming "STAR_by_teacher" is your data frame

# Load necessary libraries
library(MASS)  # For stepAIC function

# Initial model with only response variable and one predictor
initial_model <- lm(Teacher_Math_Score ~ Class_Type_Grade_1, data = STAR_by_teacher)

# Fit full model with all potential predictors except Teacher_ID
full_model <- lm(Teacher_Math_Score ~ . - Teacher_ID, data = STAR_by_teacher)

# Perform stepwise selection using AIC
final_model <- stepAIC(initial_model, direction = "both", scope = list(lower = initial_model, upper = full_model), trace = FALSE)

# Print selected model
print(final_model$call)
```

The AIC selection process yields the same model we've used throughout this project, supporting our suspicions that class type and school are the best predictors of math score outcomes, at a teacher level. However, this AIC selection process only answers the question at a teacher level, but we should ask how/what can improve an individual student's outcome based on the data.

# Discussion 

## Causal Inference

From this study, it is clear the goal of the design is to infer causality of improvement of scores based on the treatments of class size. If the following assumptions are examined to be reasonable in the study, then causal inference can be made. The assumptions are listed and discussed below.

**1) Association:** There must be an observed association between the cause (independent variable) and the effect (dependent variable). This association can be established through statistical analysis.

**2) Randomization:** There must be randomly assigned groups of all variables. In this case, the students and teachers were randomly assigned in Kindergarten, but had the option to switch between classes upon request. The switching is visualized in Figure 7.1, where the majority of switching happens between regular and regualar+aide class sizes. This assumption is clearly violated, but since there are very few switching into or out of small class size type, it is possible to still make inferences about the small class size vs. others. However, there is no randomization of urbanicity between students and teachers, since students and teachers were not randomly moved into new locations for this experiment. An additional exploration of the urbanicity is attached in section 8.

**3) Temporal Order:** The cause must precede the effect in time. In this case, the students tested their math scores in Spring, occurring after treatment of class size.

**4) Double Blind:** Students and teachers should not have any preconception that there is a treatment effect among class types. 

**5) Consistency:** The relationship between the cause and effect should be consistent across different populations, settings, and circumstances. There is a sufficient sample of many populations in this dataset.

**6) Positivity** Any individual must have a positive probability of receiving all levels of the treatment. Since all students and teachers are randomly assigned, positivity is met.

**7) Ignorability or Exchangeability:** This is an assumption often used in observational studies where all confounding variables are measured and adjusted for, or if not measured, are exchangeable between treatment groups. Since it is impossible to measure all possible confounding variables, we will be using the association of the treatment and outcome to draw some causal inferences.

**8) Sufficient Variation:** There must be sufficient variation in the cause to observe its effect on the outcome. There is sufficient variation between class size groups.

**9) Stable Unit Treatment Value Assumption (SUTVA):** This assumption states that the treatment assigned to one unit does not affect the outcomes of other units, and there is only one version of each treatment. This statement is difficult to prove since the teaching methods between classrooms will vary. For the purposes of this analysis, we will assume that treatments are consistent and can be solely attributed to the size alone.

<center>
<h4><strong>**Figure 7.1: Alluvial from Kindergarten to First Grade**</strong></h4>
</center>
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=7, fig.height=3}
star$gkclasstype <- factor(star$gkclasstype, labels = c("small", "regular", "regular + aide"))
star$g1classtype <- factor(star$g1classtype, labels = c("small", "regular", "regular + aide"))
library(alluvial)
columns_long <- c("gkclasstype", "g1classtype")
data_long <- star[, columns_long]

# Drop Missing Values
nona_long=na.omit(data_long)

# Count how many student enrolls in each combination of class types for 3 years
allu <- nona_long %>% group_by(gkclasstype, g1classtype) %>%summarise(Freq = n())  

alluvial(allu[,1:2], freq=allu$Freq)
```

### Potential Violations of Causal Inference Assumptions

While the assumptions 2 and 9 can be argued as violated within the experiment design, we believe that it is reasonable to overlook these violations for these reasons: 1) the conclusion yielded with small class type being the highest scoring, and the majority of the small class types remained as their original assignment and 2) sample size was large enough that even with inconsistent treatment types across the 70+ schools, there should be enough similarities that conclusions can still be drawn from this data. Another criticism is a large amount of data was removed due to missingness, which would violate assumption 7 of exchangeability, but within our research there is no reason to believe there was missing data for any specific purpose other than randomness.

### Student Level vs. Teacher Level

Evaluating students for their scores instead of the teacher level was strongly considered prior to the final project, since the data is more directly obtained. However, the causal statement of students performing better in math because of class type would only be made possible if there were truly randomized design, which there was not, and assumption 9 (SUTVA) would also be violated since students are prone to peer influences, which would complicate the approaches and assumptions similarly. While it is possible that teachers affect each other's classrooms within the same school, we find that the assumption that teachers do not affect each other is a far more reasonable one than students not affecting one another. 

By implementing a randomized block design by adding school as a factor, which was shown as necessary in Figure 4.4, we treated subjects within each block to be randomly assigned to different treatment, producing better estimates of the treatment of class type. With the two way ANOVA model, we account for both sources of variability from the treatment itself and the block variability of which school a student is at. A question of interest is if urbanicity also plays a role of that, which is looked into further in section 8, but was excluded from the primary report since students had no choice of where they live and the experiment designers could not change that. 

## Conclusion

Our analysis of the STAR project underscores the pivotal role of class size in shaping student achievement, particularly in the context of first-grade math scores. Despite challenges such as imperfect randomization, our findings strongly support the efficacy of small class sizes in improving academic outcomes.

While acknowledging limitations in experimental design, our robust statistical analysis provides clear evidence of the benefits associated with smaller class sizes. These findings carry significant implications for educational policy and practice, suggesting that reducing class sizes could lead to tangible improvements in student learning.

By navigating complexities and leveraging advanced methodologies, we have uncovered valuable insights that contribute to our understanding of class size dynamics and their impact on student performance. Moving forward, it is imperative to refine methodologies and address limitations to further enhance the effectiveness of educational interventions.

In conclusion, while the STAR project has its constraints, its findings offer actionable guidance for enhancing educational outcomes. With a focus on evidence-based strategies, we can continue to drive positive change and foster academic excellence in our schools.

# Extension of Exploration
## Exploratory Data Analysis on Urbanicity

An additional question of interest brought about in class was if urbanicity should be included in the model. This section will look into if it is reasonable to include it in a model to discuss the treatment's efficacy or not, in brief. 

```{r, results='hide',echo=FALSE, message=FALSE}
# Cleanup data
star <- read_sav("STAR_Students.sav")
```

<center>
<h4><strong>**Figure 8.1: Boxplot of Math Scores vs School's Urbanicity and Class Type**</strong></h4>
</center>

```{r regression_1, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=7, fig.height=3}
# Select columns of interest
columns <- c("g1classtype", "g1schid", "g1surban", "g1tchid", "g1tgen", "g1trace", "g1thighdegree", "g1tcareer", "g1tyears", "g1tmathss")
data <- star[, columns]

# Drop observations with missing teacher id
boxdata <- data[!is.na(data$g1tchid),]

# Drop schools that do not have all 3 class types
drop_school <- c(244728, 244796, 244736, 244839)
boxdata <- boxdata[!(boxdata$g1schid %in% drop_school),]

# Drop observations without math scores
boxdata <- boxdata[!is.na(boxdata$g1tmathss),]

# Group by teacher id and calculate mean for each variable
STAR_urban <- boxdata %>%
  group_by(g1tchid) %>%
  summarize_each(funs(mean))

# Recode g1classtype
STAR_urban$g1classtype[STAR_urban$g1classtype == 2] <- 0    
STAR_urban$g1classtype[STAR_urban$g1classtype == 3] <- 2
STAR_urban$g1classtype <- factor(STAR_urban$g1classtype, levels=c(0, 1, 2), labels=c("Regular", "Small", "Regular+aide"))

# Recode g1tcareer
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 2] <- 0     
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 3] <- 1    
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 5] <- 2
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 6] <- 3
STAR_urban$g1tcareer <- factor(STAR_urban$g1tcareer)

# Recode g1tgen
STAR_urban$g1tgen[STAR_urban$g1tgen == 1] <- 0
STAR_urban$g1tgen[STAR_urban$g1tgen == 2] <- 1
STAR_urban$g1tgen <- factor(STAR_urban$g1tgen)

# Recode g1trace
STAR_urban$g1trace[STAR_urban$g1trace == 1] <- 0
STAR_urban$g1trace[STAR_urban$g1trace == 2] <- 1
STAR_urban$g1trace <- factor(STAR_urban$g1trace)

# Recode g1tcareer
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 1] <- 0     
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 2] <- 1    
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 3] <- 2
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 4] <- 3
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 5] <- 4
STAR_urban$g1tcareer[STAR_urban$g1tcareer == 6] <- 5
STAR_urban$g1tcareer <- factor(STAR_urban$g1tcareer)

# Recode g1surban
STAR_urban$g1surban <- factor(STAR_urban$g1surban, levels=c(1, 2, 3, 4), labels=c("Inner City", "Surban", "Rural", "Urban"))

# Create boxplot using ggplot2
ggplot(STAR_urban[!is.na(STAR_urban$g1surban),], aes(x=g1surban, y=g1tmathss, fill=g1classtype)) +
  geom_boxplot() +
  labs(x="Area", y="Math Test Scores", fill="Class Type") +
  scale_fill_manual(values=c("skyblue", "forestgreen", "lightblue")) +
  theme_minimal()
```

At the start of our investigation, we're focusing on why we chose the school as the main factor. We think the location of the school is crucial. Looking at the boxplot in Figure 8.1, we see a trend: rural schools tend to have higher math scores, while inner-city schools have lower scores.

We will carry out a Levene Test to see if the variances of mean math scores at a teacher level differ in variances.

```{r, echo=FALSE}
# Calculate the variances for each group:
vars = tapply(STAR_by_teacher$Teacher_Math_Score,STAR_by_teacher$School_Urbanicity,var)

alpha=0.05;

STAR_by_teacher$res.abs=abs(anova.fit$residuals);
summary(aov(res.abs~School_Urbanicity,data=STAR_by_teacher))
```
The Levene test suggests there is not enough evidence to conclude the mean math scores have different variances across urbanicities, albeit very close to the rejection zone. 

According to the project details, inner-city schools typically have more students on free or reduced-cost lunch, suggesting a lower socioeconomic background. These schools are often in urban areas. So, using the school as a factor helps us control for these differences in location and background, making our analysis more accurate. Upon deeper research, we find that controlled for School identification removes most of the variability attributed to school location/urbanicity, which is why it was left out of the main project. However, it would be reasonable to replace School_ID with Urbanicity of school and come to similar conclusions. 


# Appendix
## Additional Coefficient Reporting
Attached below (Table 9.1) is the full list of coefficients from the final model in section 5.

<center>
<h4><strong>**Table 9.1: Coefficients from Final Model including School ID**</strong></h4>
</center>

```{r, echo=FALSE}
anova.fit$coefficients

# Collect coefficients after the first three coefficients
additional_coefficients <- anova.fit$coefficients[-(1:3)]
```

<center>
<h4><strong>**Figure 9.1: Histogram of School Coefficients**</strong></h4>
</center>


```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.align="center", fig.width=6, fig.height=4}
# Create a histogram of the additional coefficients
hist(additional_coefficients, main = "", xlab = "Coefficients")
```

Figure 9.1 demonstrates a large amount of variation between the coefficients, suggesting that including school ID in the main, final model is a good choice, since simply attending a different school is associated with up to an 80 math score difference. 

# Acknowledgement {-}

Discussed project methodology in Initial Analysis with Alan Phan, Sandeep Nair and Wade Klein. All code and writings are my own or adapted from lecture.

# Reference {-}

(1) Achilles, C. M., Bain, H. P., Bellott, F., Boyd-Zaharias, J., Finn, J., Folger, J., Johnston, J., & Word, E. (2008). Tennesseeâ€™s Student Teacher Achievement Ratio (STAR) project [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/SIWH9F

# Session info {-}

<span style='color:blue'>
Report information of your `R` session for reproducibility. 
</span> 


```{r}
sessionInfo()
```